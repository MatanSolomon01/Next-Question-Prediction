# Next Question Prediction: Enhancing Human Matcher Performance with Personalized Dynamic Question Sequencing
## Matan Solomon
## Supervision: Prof. Avigdor Gal

Schema matching is a core data integration task, focusing on identifying correspondences among attributes of multiple schemata.
Although numerous algorithmic approaches were suggested for schema matching over the years, humans still participate in the process.
The reliance on human involvement is rooted in the assumption that human judgment and understanding surpass algorithmic capabilities. Traditionally, domain experts were involved, under which this assumption certainly holds. However, experts' limited and costly availability on the one hand, and the need for large amount of annotated data on the other hand led to the use of crowdsourcing platforms as a cheap and accessible alternative, challenging the validity of assumptions on human input quality. In recent years, a new research direction has investigated the capabilities and behavior of humans while performing matching tasks. Previous works utilized this knowledge to predict, and even improve, the performance of human matchers in crowdsourcing platforms. In this work, we continue this line of research by addressing an important aspect that has not been discussed so far. Current crowdsourcing platforms present the participants with a fixed predetermined series of matching questions. The fixed structure may not be the optimal solution.
Recognizing the potential suboptimality, we present and formulate the problem of dynamic tailoring of the question sequence to the learning process of the participant. We further suggest a strategy to personalize the question sequence, and by doing so, optimize the performance of the participant. The strategy of our solution is based on a deep learning model that characterizes the user in each step of the task, and predicts the user's response to potential next (yet unseen) questions. Finally, we suggest an LLM-based framework to test the efficiency of the proposed strategy, using GPT \gptVersion to emulate human participants.
